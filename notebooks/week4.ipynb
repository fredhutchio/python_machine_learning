{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portions of today's lesson were adapted from materials developed by Jack Bennetto and Matt Drury, \n",
    "# and from Jordi Warmenhoven's python solutions to ISRL labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fredhutch.io -- Intermediate Python: Machine Learning\n",
    "Fred Hutchinson Cancer Research Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 part 1 -- Dimensionality Reduction with PCA\n",
    "Last week, we continued our study of supervised learning by looking at logistic regression and random forests in regression and classification. In part 1 of this week, we'll move over to unsupervised learning, considering first why we might want to represent a dataset using fewer dimensions than the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of part 1 of today's class, you should be able to:\n",
    "\n",
    " * Explain why we might want to create a lower-dimension representation of a dataset.\n",
    " * Explain the curse of dimensionality\n",
    " * Explain the conceptual basis of PCA (principal component analysis) and what is meant by subsequent PCs\n",
    " * Describe how PCA is used in practice and when to avoid using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughout we've scattered pairs of cells like the 2 immediately below. Use them to note your thoughts, answers to questions, and the code you're experimenting with.\n",
    "(remember that you can change the type of a cell by going into command mode (cell highlighted in blue) and pressing `m` for markdown and `y` for code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider:** The dimensionality of an object is the number of independent measurements that are needed to fully describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, data on the **BMI** (body mass index) of a population sample might only include features regarding height and weight, resulting in a low dimensionality. In contrast, data from an **RNA-Seq** workflow might reasonably result in a gene expression features numbering in the hundreds of thousands, making the dimensionality of that dataset very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a dataset has number of features which overlap or describe the related aspects, we might be able to *reduce the dimensionality of the problem* and in the process discovered a simpler description of the *same thing* or *very close to it*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider why we might want to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import svd, eig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import cm\n",
    "from sklearn.datasets import load_iris, fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Always make it pretty.\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason to Reduce Dimensions #1:\n",
    "\n",
    "To express a problem or describe an object in a more natural way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Scatterplots in Cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **curse of dimensionality** is an unintuitive geometric phenomenon in high dimensional spaces.  Roughly it says\n",
    "\n",
    "> The number of points needed to evenly sample an N-dimensional cube grows exponentially with N.\n",
    "\n",
    "It sometimes helpful to say this in reverse\n",
    "\n",
    "> If you have a fixed number of points of increasing dimension (i.e. you describe the points with more and more measurements), the average distance between the points grows with the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, let's sample a fixed number of points from unit hypercubes of increasing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "dimensions = range(1, 10)\n",
    "\n",
    "# samples[dim] is a numpy array with shape (n_points, dim)\n",
    "samples = {}\n",
    "for dim in dimensions:\n",
    "    samples[dim] = np.random.uniform(size=n_points*dim).reshape((n_points, dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these samples, lets measure how much of the data lies inside a small cube of increasing size centered at a corner of the unit cube.\n",
    "\n",
    "Like this for our two-dimensional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_points_within(sample, x):\n",
    "    \"\"\"Count the number of points in sample within a distance x of \n",
    "    the origin in each direction.\n",
    "    \"\"\"\n",
    "    within = sample <= x\n",
    "    all_within = np.all(within, axis=1)\n",
    "    return np.sum(all_within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "two_dim_data = samples[2]\n",
    "ticks = np.linspace(0.1, 1, num=20)\n",
    "\n",
    "ax = axs[0]\n",
    "ax.scatter(two_dim_data[:, 0], two_dim_data[:, 1])\n",
    "for edge_len in reversed(ticks):\n",
    "    ax.add_patch(patches.Rectangle((0, 0), edge_len, edge_len, \n",
    "                                   color=cm.gray(0.25 + 0.5*edge_len), \n",
    "                                   linewidth=3, fill=False, alpha=0.5))\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylim((0, 1))\n",
    "ax.set_title(\"Uniformly Sample Points in Unit Square\")\n",
    "\n",
    "ax = axs[1]\n",
    "density = [num_points_within(two_dim_data, edge_len) for edge_len in ticks]\n",
    "ax.plot(ticks, density)\n",
    "ax.set_title(\"Number of Points within Sub-Square\")\n",
    "ax.set_xlabel(\"Side Length of Sub-Square\")\n",
    "ax.set_ylabel(\"N Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve on the right shows how the number of points in a square changes as the size of the square increases to envelop the *entire* large square.\n",
    "\n",
    "Not let's run this simulation but **change the dimension** of the larger square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "\n",
    "ticks = np.linspace(0.1, 1, num=25)\n",
    "\n",
    "for dim, sample in samples.items():\n",
    "    density = [num_points_within(sample, edge_len) for edge_len in ticks]\n",
    "    ax.plot(ticks, density, label=\"Dimension {}\".format(dim), \n",
    "            color=cm.BuPu(0.5 + dim/20.0))\n",
    "    \n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_title(\"Number of Points Within a Sub-Square\")\n",
    "ax.set_xlabel(\"Side Length of Sub-Square\")\n",
    "ax.set_ylabel(\"N Points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In larger dimensions fewer and fewer of those data are *near* the fixed corner of the cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For algorithms that make predictions at $x$ by looking for training samples that are *near* $x$, we need an increasingly vast amount of data to construct accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What algorithms we have studied use only points *near* $x$ to make predictions or inferences about $x$.  Which do not have this property?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason to Reduce Dimensions #2:\n",
    "\n",
    "To combat the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Visualization of Data\n",
    "\n",
    "In Fischer's classic iris dataset, we describe flowers by four measurements, so it is a four dimensional representation of the flowers.\n",
    "\n",
    "![Iris Measurements](../img/iris-measurements.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "iris_type = iris.target\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data of dimension larger than *two* leads to challenges for visualization.  In dimension three we can draw some plots, but they are hard to use.  In dimension four, we need **coping strategies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One option** is to plot each feature against each other feature in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = scatter_matrix(iris_data, \n",
    "                   c=np.array([\"red\", \"green\", \"blue\"])[iris_type], \n",
    "                   figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** How can we construct *one* scatterplot of the four-dimensional data that is in some sense the *best* way to represent it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason to Reduce Dimensions #3:\n",
    "\n",
    "To compensate for our inability to visualize high dimensional situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think of our data as a point cloud in $p$-dimensional space, and ask the following question:\n",
    "\n",
    "**Fundamental Question:** How can we find a 1-dimensional data set $X_1$ so that\n",
    "\n",
    "  - Going from $X$ to $X_1$ is a very simple operation.\n",
    "  - In some sense, $X_1$ is the *best* one dimensional reconstruction of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Consider \"Going from $X$ to $X_1$ is a very simple operation\".  What are some good candidates for *simple operations*.  The following animation may be suggestive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pca Animation](../img/pca.gif)\n",
    "\n",
    "Image source: http://stats.stackexchange.com/a/140579/74500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Dimensional PCA\n",
    "\n",
    "We look for a **line** so that the *projection* of the data $X$ onto that line\n",
    "\n",
    "  - Results in points minimizing the total orthogonal (perpendicular) squared distance to $X$\n",
    "  - Results in points with *maximum variance* as a 1-dimensional data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data_set(n=n_points):\n",
    "    theta = np.random.uniform(low=0, high=2*3.14159)\n",
    "    rotation = np.array([[np.cos(theta), np.sin(theta)], \n",
    "                         [-np.sin(theta), np.cos(theta)]])\n",
    "    data = np.column_stack([np.random.normal(size=n), 2*np.random.normal(size=n)])\n",
    "    rotated_data = data @ rotation\n",
    "    return rotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1dim_pca(ax):\n",
    "    \"\"\"Make a plot of:\n",
    "        - A point cloud rotated to an random angle (blue).\n",
    "        - The principal component line through the orgin (green).\n",
    "        - Plot the eigenvector representing the principal component (black).\n",
    "        - The point cloud projected onto the principal component line (green).\n",
    "    \"\"\"\n",
    "    X = random_data_set()\n",
    "    pcd = PCA(1).fit(X)\n",
    "    e = pcd.components_[0]\n",
    "    # Plot the data set.\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    # Plot a line for the principal component.\n",
    "    x = np.linspace(-10, 10, num=3)\n",
    "    ax.plot(e[0]*x, e[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    # Plot the projections of the data points onto the line.\n",
    "    X_proj = X @ e\n",
    "    X_reconst = np.array([t*e for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot an arrow for the first principal direction.\n",
    "    ax.arrow(0, 0, e[0], e[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.set(adjustable='box', aspect='equal')\n",
    "    ax.set_xlim((-5, 5))\n",
    "    ax.set_ylim((-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    plot_1dim_pca(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots, we superimpose the first principal component onto a random scatter plot.  Notice that:\n",
    "\n",
    "> The green line is chosen so that the projections of the data points onto this line are **maximally spread out**.\n",
    "\n",
    "The direction of this line is called the **first principal component** of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** How should we find the **second** principal component of the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general** PCA applied to a dataset $X$ returns the best line, plane, 3-space, 4-space, ... so that when $X$ is projected into the subspace\n",
    "\n",
    "  - The total squared distance from the original data to the projections in minimized.\n",
    "  - The total variance (i.e. the sum of the variances in all posible orthogonal directions) of the projected dataset is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2dim_pca(ax):\n",
    "    \"\"\"Make a plot of:\n",
    "        - A point cloud rotated to an random angle (blue).\n",
    "        - The principal component lines through the orgin (green).\n",
    "        - The eigenvectors representing the principal components (black).\n",
    "        - The point cloud projected onto the principal component lines (green).\n",
    "    \"\"\"\n",
    "    X = random_data_set()\n",
    "    pcd = PCA(2).fit(X)\n",
    "    e_1 = pcd.components_[0]\n",
    "    e_2 = pcd.components_[1]\n",
    "    # Plot the data set.\n",
    "    ax.scatter(X[:, 0], X[:, 1])\n",
    "    # Plot a lines for the principal components.\n",
    "    x = np.linspace(-10, 10, num=3)\n",
    "    ax.plot(e_1[0]*x, e_1[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    ax.plot(e_2[0]*x, e_2[1]*x, color='green', alpha=0.3, linestyle='-')\n",
    "    # Plot the projections of the data points onto the first line.\n",
    "    X_proj = X @ e_1\n",
    "    X_reconst = np.array([t*e_1 for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot the projections of the data points onto the second line.\n",
    "    X_proj = X @ e_2\n",
    "    X_reconst = np.array([t*e_2 for t in X_proj])\n",
    "    ax.scatter(X_reconst[:, 0], X_reconst[:, 1], color=\"green\", alpha=0.5)\n",
    "    # Plot an arrow for the first principal direction.\n",
    "    ax.arrow(0, 0, e_1[0], e_1[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.arrow(0, 0, e_2[0], e_2[1], head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "    ax.set(adjustable='box', aspect='equal')\n",
    "    ax.set_xlim((-5, 5))\n",
    "    ax.set_ylim((-5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    plot_2dim_pca(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plots, we superimpose the first and second principal components onto a random scatter plot.  Notice that:\n",
    "\n",
    "  - The **first principal component** determines a green line that maximizes the variance of the data's projection.\n",
    "  - The **second principal component** is orthogonal to the first, and maximizes the projection of the \"leftover\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Later Consideration: Computing Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the problem of finding principal components involves, somewhat surprisingly, the *eigenvalues* and *eigenvectors* of the covariance matrix $\\frac{1}{n} X^t X$ of $X$.\n",
    "\n",
    "**Note:** X must be centered before computing the covariance in this manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup:\n",
    "\n",
    "$X$ is a dataset, which we represent as a $n \\times p$ matrix of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random_data_set(n=50)\n",
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Center the Matrix.\n",
    "\n",
    "**Centering** the matrix is the process of subracting the column means from the columns themselves.  This results in a new matrix with column means zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - np.mean(X, axis=0)\n",
    "print(np.mean(X_centered, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the sample covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix of the sample is $M = \\frac{1}{n} X^t X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = X.T @ X * (1/float(X.shape[0]))\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal terms of the covariance matrix are the variance of the first and second columns. The cross terms show the interactions, showing how much the mean of one feature changes with a change in the mean of another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute the Eigenvectors and Eigenvalues of M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors, when ordered in order of decreasing eigenvalue, are the principal components of $X$.\n",
    "\n",
    "**Note:**\n",
    "  - Since $M$ is a symmetric, it has a full set of $p$ eigenvectors.\n",
    "  - Since $M$ is non-negative definite,  the eigenvalues are non-negative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_values, e_vectors = eig(M)\n",
    "print(\"The eigenvectors of M are:\")\n",
    "print(e_vectors)\n",
    "print(\"The eigenvlaues of M are {}\".format(e_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Later Consideration: Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **eigenvectors** of $M$ are vectors $v$ that satisfy a relationship like:\n",
    "\n",
    "$$ M v = \\lambda v $$\n",
    "\n",
    "I.e., the matrix $M$ acts as a **scaling** on the vector $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(M @ e_vectors[:, 0]) / e_vectors[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(M @ e_vectors[:, 1]) / e_vectors[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 6), )\n",
    "\n",
    "coord_vectors = np.array([[1, 0], [0, 1]])\n",
    "coord_image = M @ coord_vectors\n",
    "e_image = M @ e_vectors\n",
    "\n",
    "# Set coordinate ranges to the maximal possible arrow position\n",
    "max_coord = max(np.max(coord_vectors), np.max(coord_image), np.max(e_image))\n",
    "for ax in axs.flatten():\n",
    "    ax.set_xlim(-max_coord - 0.5, max_coord + 0.5)\n",
    "    ax.set_ylim(-max_coord - 0.5, max_coord + 0.5)\n",
    "    ax.set(adjustable='box', aspect='equal')\n",
    "\n",
    "\n",
    "# Plot the coordiante vectors and thier images.\n",
    "axs[0, 0].arrow(0, 0, coord_vectors[0, 0],  coord_vectors[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[0, 0].arrow(0, 0, coord_vectors[0, 1], coord_vectors[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[0, 0].set_title(\"Coordinate Vectors\")\n",
    "axs[0, 1].arrow(0, 0, coord_image[0, 0],  coord_image[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[0, 1].arrow(0, 0, coord_image[0, 1], coord_image[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[0, 1].set_title(\"$M \\\\times$ Coordinate Vectors\")\n",
    "\n",
    "\n",
    "# Plot the eigenvectors and thier images.\n",
    "axs[1, 0].arrow(0, 0, e_vectors[0, 0],  e_vectors[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[1, 0].arrow(0, 0, e_vectors[0, 1], e_vectors[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[1, 0].set_title(\"Eigenvectors\")\n",
    "axs[1, 1].arrow(0, 0, e_image[0, 0],  e_image[1, 0],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='b', ec='b')\n",
    "axs[1, 1].arrow(0, 0, e_image[0, 1], e_image[1, 1],\n",
    "                head_width=0.1, linewidth=3, head_length=0.2, fc='g', ec='g')\n",
    "axs[1, 1].set_title(\"$M \\\\times$ Eigenvectors\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the effect of multiplying the matrix $M$ by coordinate vectors is difficult to understand, the coordinate vectors are rotated and scaled, each by a different amount.\n",
    "\n",
    "On the other hand, the eigenvectors of $M$ are much better behaved when multiplied by $M$, the direction of the eigenvectors are preserved, but they are stretched or shrunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the eigenvectors of $M$ on top of the centered data set to see how they relate to the scatterplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "\n",
    "ax.scatter(X_centered[:, 0], X_centered[:, 1])\n",
    "ax.arrow(0, 0, e_vectors[0, 0], e_vectors[1, 0],\n",
    "         head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "ax.arrow(0, 0, e_vectors[0, 1], e_vectors[1, 1],\n",
    "         head_width=0.33, linewidth=3, head_length=0.5, fc='k', ec='k')\n",
    "_ = ax.set(adjustable='box', aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "  - The Principal Components algorithm computes the *eigenvectors* and *eigenvalues* of the matrix $X^t X$.\n",
    "  - Each eigenvector is called a *principal component* of $X$.\n",
    "  - Projecting onto the first $k$ principal components creates the **best k dimensional reconstruction of the data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can use `sklearn.decomposition.PCA` to find principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = PCA(2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn returns a `PCA` returns an object with a `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `components_` attribute is an orthogonal matrix containing (as rows) the eigenvalues of the correlation matrix.  That is, the *principal components*.\n",
    "\n",
    "Let's call the matrix of principal components $E$.\n",
    "\n",
    "### Properties of Principal Components\n",
    "\n",
    "1. Taking the first $k$ rows of $E$ gives (a basis for) the \"best\" $k$ dimensional subspace.  We call this subset matrix $E_{1:k}$.\n",
    "\n",
    "2. \"Best\" above means: projecting the dataset onto this subspace preserves the **most variance in the data** out of all possible such projections.\n",
    "\n",
    "3. The matrix multiplication $XE_{1:k}^t$ gives the *coefficients* of the \"best\" reconstruction of $X$ in the basis $E_k$.\n",
    "\n",
    "4. The matrix multiplication $(X E_{1:k}^t) E_{1:k}$ gives the \"best\" reconstruction of $X$ as a $k$ dimensional object.  That is, this reconstructs $X$ as a matrix with $k$ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the `explained_variance_` attribute contains the total variance of the *projected* dataset.\n",
    "\n",
    "Each of the numbers in `explained_variance_` is an *eigenvalue* of the covariance matrix $X^t X$.\n",
    "\n",
    "5. The **eigenvalues** of the covariance matrix measure the variance of the projection of the data onto the associated eigenvector (principal component)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using the $X$ data above, what fraction of the variance is explained by the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Principal Component Analysis IS For\n",
    "\n",
    "So, **the purpose of PCA is to approximately reconstruct data sets as a lower dimensional object.**\n",
    "\n",
    "Applications of this include:\n",
    "\n",
    "  - Visualization.\n",
    "  - Clustering.\n",
    "  - Data Compression.\n",
    "  - Mathematical analysis of the properties of regression modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: The Best Scatterplot of Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a \"best\" scatterplot of the iris dataset using PCA to project\n",
    "\n",
    "```\n",
    "Four dimensional iris data => Best two dimensional reconstruction\n",
    "                           => Best two dimensional scatterplot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the above discussion we are plotting the *expression of $X$ in the principal component basis*.\n",
    "\n",
    "$$ X E_{1:2}^t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = PCA(2).fit(iris_data.values)\n",
    "iris_reduced = iris_data.values @ pcd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "\n",
    "ax.scatter(iris_reduced[:, 0], iris_reduced[:, 1], \n",
    "           color=np.array([\"red\", \"green\", \"blue\"])[iris_type])\n",
    "ax.set_title(\"Scatterplot of Iris Data in PCA 2-Plane\")\n",
    "ax.set_xlabel(\"First Principal Component\")\n",
    "ax.set_ylabel(\"Second Principal Component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Does PCA find the plane in which the different categories are the most clearly distinguished?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Eigenfaces: Reconstructing Faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of *very* high dimensional data, we introduce the `faces` data set.\n",
    "\n",
    "An extended version of this example is available in the sklearn docs here: http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=154)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# Global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# Local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2.0 * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(\"Centered Olivetti faces\", faces_centered[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these images is stored as a numpy array.\n",
    "\n",
    "Each entry in the array measures *one* pixel intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_face = faces_centered[0].reshape(image_shape)\n",
    "print(first_face)\n",
    "plt.imshow(first_face, cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't believe me?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_face_ascii = np.array(['#', '.'])[(first_face >= 0).astype(int).flatten()].reshape(image_shape)\n",
    "print('\\n'.join([''.join(row) for row in first_face_ascii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a $64 \\times 64$ array, and so is represented as a $4096$ dimensional object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Do you really believe that faces are $4096$ dimensional objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply PCA to the faces dataset to lower the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_pcd = PCA(100).fit(faces_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the $E^t$ matrix (i.e. `faces_pcd.components_.T`) are called **eigenfaces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "for ax, i in zip(axs.flatten(), range(6)):\n",
    "    eigenface = faces_pcd.components_[i, :].reshape(image_shape)\n",
    "    ax.imshow(eigenface, cmap=cm.gray)\n",
    "    ax.set_title(\"{}'th Eigenface\".format(i))\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What facial features seem to be captured in the first few eigenfaces?  Why do you think this may be so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our reconstruction procedure with the face data.  This lets us create **smaller dimensional faces**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_face_data(n_dim):\n",
    "    eigenvalues = faces_pcd.components_[:n_dim, :].T\n",
    "    faces_reduced = faces_centered @ eigenvalues @ eigenvalues.T\n",
    "    return faces_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(13, 6))\n",
    "\n",
    "for ax, dim in zip(axs.flatten(), [1, 2, 4, 10, 25, 50]):\n",
    "    reduced_data = reduce_face_data(dim)\n",
    "    first_face = reduced_data[0].reshape(image_shape)\n",
    "    ax.imshow(first_face, cmap=cm.gray)\n",
    "    ax.set_title(\"Face Reconstructed with {} PCA\\nComponents\".format(dim))\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** When applying PCA to reduce the dimensionality of a data set, we have to choose a *number of dimensions* to keep.  What kind of concerns should we consider when choosing this number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Principal Componenets Analysis is NOT for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an, unfortunately, popular method which combines PCA with regression, with the intent of improving the generalization properties of the regression.  It is called **Principal Component Regression** and it goes like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**: You have a matrix $X$ and a response $y$, and you want to fit a regression to predict $y$ from $X$.\n",
    "\n",
    "**Procedure**:\n",
    "1. Do PCA on $X$, let $E$ be the matrix of principal components.\n",
    "2. Discard some of the principal components, get the matrix $E_{1:k}$.\n",
    "3. Regress $y$ on $E_{1:k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Critique this procedure, pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details on why **not** to use selection of principal components as a regularization strategy in regression [see this paper](http://www.uvm.edu/~rsingle/stat380/F04/possible/Hadi%2BLing-AmStat-1998_PCRegression.pdf) and [this paper](http://automatica.dei.unipd.it/public/Schenato/PSC/2010_2011/gruppo4-Building_termo_identification/IdentificazioneTermodinamica20072008/Biblio/Articoli/PCR%20vecchio%2082.pdf).\n",
    "\n",
    "[This question](http://stats.stackexchange.com/questions/101485/examples-of-pca-where-pcs-with-low-variance-are-useful) gives real life examples of data sets where PCR fails because $y$ is only related to the **low variance** principal components.\n",
    "\n",
    "The moral of this story: PCA is reasonable as a variable creation strategy only when you have a good reason to believe that there are low variance components in $X$ that will *not* contribute to prediction.\n",
    "\n",
    "Depending on the context, the above assumption could be reasonable or unreasonable, meaning careful consideration of the problem space is needed to determine if PCA should be used to select features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Why is PCA **not** a variable selection algorithm? What circumstances would allow variable selection by PCA to be a reasonable approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 part 2 -- Clustering: k-means, hierarchical, and more!\n",
    "In part one of this week, we looked at reasons why we would want to reduce the dimensionality of a dataset and the implications of doing so using **PCA**. In part 2, we will look at **Clustering**, another common unsupervised learning method that often comes next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of part 2 of today's class, you should be able to:\n",
    "\n",
    "* Implement a **k-means** algorithm for clustering\n",
    "* Discuss how **curse of dimensionality** affects clustering\n",
    "* Choose the best k using the **elbow method** or **silhouette scores**\n",
    "* Implement and interpret **hierarchical clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Problem\n",
    "\n",
    "The goal of clustering is to divide the data into **distinct subgroups** such that observations within each group are similar.\n",
    "\n",
    "![](img/clusters.png)\n",
    "\n",
    "\n",
    "## Various Algorithms\n",
    "\n",
    "There are several approaches to clustering, each with variations.\n",
    "\n",
    "* k-means clustering\n",
    "* Hierarchical clustering\n",
    "* Density-based clustering (DBSCAN)\n",
    "* Distribution-based clustering\n",
    "* ...\n",
    "\n",
    "How do we measure how good the clustering is?\n",
    "\n",
    "## Within-Cluster Sum of Squares\n",
    "\n",
    "Measures the goodness of a clustering\n",
    "\n",
    "$$W(C) = \\sum_{k=1}^{K} \\frac{1}{K} \\sum_{C(i)=k}  \\sum_{C(j)=k} || x_i - x_j ||^2 $$\n",
    "\n",
    "where $K$ is the number of clusters, $C(i)$ is the cluster label of point $i$, and $x_i$ is the position of point $i$.\n",
    "\n",
    "Do you need to normalize/standardize the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means Algorithm\n",
    "\n",
    "The k-means algorithm involves repeatedly assigning points to clusters and then finding new clusters based on those points.\n",
    "\n",
    "* Choose a number of clusters k\n",
    "* 1\\. Choose initial clusters\n",
    "* 2\\. Repeatedly:\n",
    "    * a\\. For each of k clusters, compute cluster *centroid* by taking\n",
    "mean vector of points in the cluster\n",
    "    * b\\. Assign each data point to cluster for which centroid is closest\n",
    "(Euclidean)\n",
    "\n",
    "...until clusters stop changing\n",
    "\n",
    "This approach can be seen as simple version of an **Expectation-Maximization** algorithm. The step of choosing a new centroid is called a *maximization* step, finding the best cluster center given some data. The assignment of data points is the *expectation* step, finding the expected cluster for each point.\n",
    "\n",
    "Discussion: how should we choose initial clusters? Does it matter?\n",
    "\n",
    "# k-means Algorithm\n",
    "\n",
    "![The k-means algorithm.](../img/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means++\n",
    "\n",
    "Consider the points below. How would you break them into two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "colors = 'k'\n",
    "ax.scatter([1, 1], [0, 1], marker='x', s=200, c=['r', 'b'])\n",
    "colors = ['r', 'b', 'r', 'b']\n",
    "ax.scatter([0, 0, 2, 2], [0, 1, 0, 1], s=200, c=colors)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple common way to choose initial clusters. One is to choose randomly cluster centers from the initial points. Another is to randomly assign points to clusters and then find the centers of those.\n",
    "\n",
    "Like many algorithms, it's easy to get caught in a local minimum. The usual way to avoid this is run the algorithm a number of times and find the best solution. `sklearn` does this automatically with the `n_init` parameter.\n",
    "\n",
    "That helps, but can we choose better starting points?\n",
    "\n",
    "k-means++ is the same algorithm as k-means but with a different initialization, attempting to find initial points near the center of different clusters.\n",
    "\n",
    " * Choose one point for first center.\n",
    " * Repeat:\n",
    "    \n",
    "    * Calculate distance from each point to the nearest center $d_i$\n",
    "    * Choose a point to be the next center, randomly, using a weighed probability $d_i^2$\n",
    "\n",
    " ... until k centers have been chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the correct number of clusters?\n",
    "\n",
    "We've talked about the \"best\" way to divide data into a given number of clusters, but what number is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [(-2, -3, .75, 100),\n",
    "            (-5, -5, .75, 75),\n",
    "            (4, 3, 1.5, 200),\n",
    "            (1, -3, .5, 150),\n",
    "            (-1, 0.15, 0.75, 100)]\n",
    "k = len(clusters)\n",
    "n = sum([c[3] for c in clusters])\n",
    "\n",
    "x1 = np.array([])\n",
    "x2 = np.array([])\n",
    "\n",
    "for c in clusters:\n",
    "    x1 = np.concatenate([x1, (stats.norm(c[0], c[2]).rvs(c[3]))])\n",
    "    x2 = np.concatenate([x2, (stats.norm(c[1], c[2]).rvs(c[3]))])\n",
    "x = np.stack((x1, x2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[:,0], x[:,1], linewidths=0, color='k')\n",
    "ax.set_xlim(xmin=-9, xmax=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do you see?\n",
    "\n",
    "Let's try fitting them with k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(5)\n",
    "y = km.fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.axis('off')\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[:,0], x[:,1], c=y, linewidths=0)\n",
    "ax.set_ylim(ymin=-9, ymax=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are basically the clusters we created the data from.\n",
    "\n",
    "# Choosing K\n",
    "\n",
    "Can we just use within-cluster sum of squares (WCSS) to choose k?\n",
    "\n",
    "\n",
    "More clusters $\\implies$ lower WCSS.\n",
    "\n",
    "There are several measures for the \"best\" k, and no easy answer\n",
    "\n",
    " * The Elbow Method\n",
    " * Silhouette Score\n",
    " * GAP Statistic\n",
    "\n",
    "First, let's cluster the data above with k-means with various values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxk = 13\n",
    "wcss = np.zeros(maxk)\n",
    "silhouette = np.zeros(maxk)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16,9))\n",
    "\n",
    "# flatten\n",
    "axes = [ax for axrow in axes for ax in axrow]\n",
    "\n",
    "for k, ax in zip(range(1,maxk), axes):\n",
    "    km = KMeans(k)\n",
    "    y = km.fit_predict(x)\n",
    "    ax.axis('off')\n",
    "    ax.scatter(x[:,0], x[:,1], c=y, linewidths=0, s=10)\n",
    "    ax.set_ylim(ymin=-9, ymax=8)\n",
    "    \n",
    "    \n",
    "    for c in range(0, k):\n",
    "        for i1, i2 in itertools.combinations([ i for i in range(len(y)) if y[i] == c ], 2):\n",
    "            wcss[k] += sum(x[i1] - x[i2])**2\n",
    "    wcss[k] /= 2\n",
    "    \n",
    "    if k > 1:\n",
    "        silhouette[k] = silhouette_score(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Method\n",
    "\n",
    "Let's start with the Elbow method. In this, we simply plot the within-cluster sum of squares and try to see what looks like an elbow.\n",
    "\n",
    "What looks best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(3,maxk), wcss[3:maxk], 'o-')\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"within-cluster sum of squares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is some metric we can maximize.\n",
    "\n",
    "What would that look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing K -- Silhouette Score\n",
    "\n",
    "For each point $x_i$:\n",
    "\n",
    " * $a(i)$ average dissimilarity of $x_i$ with points in the same cluster\n",
    " * $b(i)$ average dissimilarity of $x_i$ with points in the nearest cluster\n",
    "    * \"nearest\" means cluster with the smallest $b(i)$\n",
    "\n",
    "$$\\text{silhouette}(i) = \\frac{b(i) - a(i)}{max(a(i), b(i))} $$\n",
    "\n",
    "What's the range of silhouette scores?\n",
    "\n",
    "The silhouette score of a clustering is the average of silhouette score of all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,maxk), silhouette[2:maxk], 'o-')\n",
    "ax.set_xlabel(\"number of clusters\")\n",
    "ax.set_ylabel(\"silhouette score\")\n",
    "#ax.set_ylim(ymin=0.0, ymax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible silhouette scores:\n",
    " * near 1: very small tight cluster.\n",
    " * 0: at the edge of two clusters; could be in either.\n",
    " * < 0: oops.\n",
    "\n",
    "The higher the the average silhouette score, the tighter and more separated the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Graph\n",
    "\n",
    "A silhouette graph is a representation of the silhouette score of every data point, grouped first by cluster and then in decreasing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = x\n",
    "range_n_clusters = range(2,10)\n",
    "\n",
    "# taken from sklearn\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = matplotlib.cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhoutte score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = matplotlib.cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors)\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Later Consideration: Choosing K -- GAP Statistic\n",
    "\n",
    "For each $K$, compare $W_K$ (within-cluster sum of squares) with that of randomly generated \"reference distributions\"\n",
    "\n",
    "Generate B distributions\n",
    "\n",
    "$$Gap(K) = \\frac{1}{B} \\sum_{b=1}^B \\log{W_{Kb}} - \\log{W_K}$$\n",
    "\n",
    "Choose smallest K such that $Gap(K) \\ge Gap(K+1) - s_{N+1}$\n",
    "\n",
    "where $s_{K}$ is the standard error of $Gap(K)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "With many clustering methods the goal is to divide the data into a specific number of clusters. With hierarchical clustering we group data points as leaves on a tree, clustering them into larger and larger groups.\n",
    "\n",
    "How would we group these?\n",
    "\n",
    "![](../img/letters-ungrouped.png)\n",
    "\n",
    "\n",
    "<table><tr><td><img src='../img/letters-grouped.png'></td><td><img src='../img/letters-dendrogram.png'></td></tr></table>\n",
    "\n",
    "\n",
    "# Hierarchical Clustering\n",
    "\n",
    "The basic algorithm of hierarchical clustering is\n",
    "\n",
    " * Assign each point to its own cluster\n",
    " * Repeat:\n",
    "\n",
    "   * Compute distances between clusters\n",
    "   * Merge closest clusters\n",
    "\n",
    " ...until all are merged\n",
    "\n",
    "How do we define dissimilarity between clusters?\n",
    "\n",
    "## Linkage\n",
    "\n",
    "It's easy to talk about the distance (or dissimilarity) between two points, but between clusters it's less clear. There are a few different measures used.\n",
    "\n",
    "* **Complete:** Maximum pairwise dissimilarity between points in clusters -- good\n",
    "* **Average:** Average of pairwise dissimilarity between points in clusters -- also good\n",
    "* **Single:** Minimum pairwise dissimilarity between points in clusters -- not as good; can lead to long narrow clusters\n",
    "* **Centroid:** Dissimilarity between centroids -- used in genomics; risk of inversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with k-means and other methods\n",
    "\n",
    "k-means has limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "r = np.concatenate([stats.norm(8,1).rvs(250),\n",
    "                    stats.norm(2,1).rvs(50)])\n",
    "a = stats.uniform(0, 6.28).rvs(300)\n",
    "x = r * np.cos(a)\n",
    "y = r * np.sin(a)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.axis('off')\n",
    "ax.scatter(x, y, color=\"r\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will k-means do here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "With DBSCAN (Density-Based Spacial Clustering of Applications with Noise) we don't specify the number of clusters. Instead we specify:\n",
    "\n",
    " * $\\epsilon$: distance between points for them to be connected\n",
    " * minPts: number of connected points for a point to be a \"core\" point\n",
    "\n",
    "A cluster is all connected core points, plus others within $\\epsilon$ of one of those. Other points are noise.\n",
    "\n",
    "Let's tackle the above problem with DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.1\n",
    "minpts = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "distances = squareform(pdist(np.stack([x,y], axis=1)))\n",
    "connected = distances < epsilon\n",
    "ax.scatter(x, y, color=\"k\", alpha=0.3)\n",
    "for i in range(len(x)):\n",
    "    for j in range(i):\n",
    "        if connected[i,j]:\n",
    "            ax.plot([x[i], x[j]], [y[i], y[j]], 'k', lw=0.5)\n",
    "\n",
    "coreindices = np.where(connected.sum(axis=0) > (minpts))[0]\n",
    "ax.scatter(x[coreindices], y[coreindices], s=150, c='r')\n",
    "\n",
    "for i in coreindices:\n",
    "    for j in coreindices:\n",
    "        if i > j and connected[i,j]:\n",
    "            ax.plot([x[i], x[j]], [y[i], y[j]], 'k', lw=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is even a hierarchical version of DBSCAN called HDBSCAN that yields excellent results by incorporating hierarchical aspects in the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Later Consideration: Distribution-based clustering\n",
    "\n",
    "With distribution-based clustering we assume some fixed number of clusters, and assume they follow some (often normal) distribution. We then try to find the parameters that have the **maximum likelihood** of producing these data.\n",
    "\n",
    "This is more difficult then other problems we've seen because we don't know which point came from which distribution. We need to add some hidden variables to the problem: the probability each point came from each distribution. We can solve this by an **expectation-maximization** (EM) algorithm in which we alternate between expectation steps (where we calculate the hidden variables) and maximization steps (in which we calculate the maximum-likelihood parameters assuming the hidden variables are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100\n",
    "actual_mus = 1, 5\n",
    "actual_sds = 1.5, 0.5\n",
    "x = np.concatenate([stats.norm(actual_mus[0], actual_sds[0]).rvs(npts//2),\n",
    "                    stats.norm(actual_mus[1] ,actual_sds[1]).rvs(npts//2)])\n",
    "y = np.ones(npts)/2.\n",
    "fig, ax = plt.subplots(figsize=(10,1))\n",
    "ax.scatter(x[:npts//2], y[:npts//2], c='b', alpha=0.1, s=100)\n",
    "ax.scatter(x[npts//2:], y[npts//2:], c='r', alpha=0.1, s=100)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first step is to make guesses for the two starting distrubitions. Let's go with N(0,1) and N(2,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = [-1, 0]\n",
    "sds = [1, 1]\n",
    "dists = [stats.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "actual_dists = [stats.norm(mu, sd) for mu, sd in zip(actual_mus, actual_sds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_em(x, y, dists, actual_dists):\n",
    "    xpts = np.linspace(-5,10,100)\n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    npts = len(x)\n",
    "    ax.scatter(x[:npts//2], y[:npts//2], c='b', alpha=0.1, s=100, label=\"sample from dist 0\")\n",
    "    ax.scatter(x[npts//2:], y[npts//2:], c='r', alpha=0.1, s=100, label=\"sample from dist 1\")\n",
    "    ax.plot(xpts, dists[0].pdf(xpts), 'b', label=\"estimated dist 0\")\n",
    "    ax.plot(xpts, 1-dists[1].pdf(xpts), 'r',  label=\"estimated dist 1\")\n",
    "    ax.plot(xpts, actual_dists[0].pdf(xpts), 'b:',  label=\"actual dist 0\")\n",
    "    ax.plot(xpts, 1-actual_dists[1].pdf(xpts), 'r:',  label=\"actual dist 1\")\n",
    "    ax.set_ylabel(\"Probability data from dist 1\")\n",
    "    ax.legend()\n",
    "\n",
    "def print_stats(actual_mus, actual_sds, mus, sds):\n",
    "    #for i in (0, 1):\n",
    "    print(\"      actual  estimate\")\n",
    "    print(\"mu_0  {0:.2f}    {1:.3f}\".format(actual_mus[0], mus[0]))\n",
    "    print(\"mu_1  {0:.2f}    {1:.3f}\".format(actual_mus[1], mus[1]))\n",
    "    print(\"sd_0  {0:.2f}    {1:.3f}\".format(actual_sds[0], sds[0]))\n",
    "    print(\"sd_1  {0:.2f}    {1:.3f}\".format(actual_sds[1], sds[1]))\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we're going to calculate the relative likelihood that each point will be in `dist[1]` (the upper distribution, as opposed to `dist[0]`, the lower one).\n",
    "\n",
    "We'll plot that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we update each distribution to be (normal) distribution that has the maximum likelihood of generating the data. We'll weight each point by the probability (calculated above) of being in that distribution.\n",
    "\n",
    "We can just use the (weighted) means and standard deviations for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_stats(x, weights):\n",
    "    wtotal = weights.sum()\n",
    "    wmean = (x * weights).sum() / wtotal\n",
    "    wsd = (((weights*(x - wmean))**2).sum() / wtotal)**0.5\n",
    "    return wmean, wsd\n",
    "\n",
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [stats.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [stats.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "plot_em(x, y, dists, actual_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "mus[1], sds[1] = weighted_stats(x, y)\n",
    "dists = [stats.norm(mu, sd) for mu, sd in zip(mus, sds)]\n",
    "\n",
    "plot_em(x, y, dists, actual_dists)\n",
    "print_stats(actual_mus, actual_sds, mus, sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    y = dists[1].pdf(x) / (dists[0].pdf(x) + dists[1].pdf(x))\n",
    "    mus[0], sds[0] = weighted_stats(x, 1-y)\n",
    "    mus[1], sds[1] = weighted_stats(x, y)\n",
    "    dists = [stats.norm(mu, sd) for mu, sd in zip(mus, sds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_em(x, y, dists, actual_dists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 part 3 -- PCA and Clustering in Action\n",
    "In parts 1 and 2 of today's class, we looked at dimensionality reduction using PCA and subsequent clustering using common techniques. In the final part of today's class, we'll investigate how these methods could be used in some important problem spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of part 3 of today's class, you should be able to:\n",
    "\n",
    " * Investigate a real world dataset using PCA.\n",
    " * Implement k-means and hierarchical clustering on a generated dataset and contast the results.\n",
    " * Run the entire suite on the NCI60 gene expression dataset included with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is part of the base R distribution, exported here as a .csv file for convenience.\n",
    "df = pd.read_csv('../data/USArrests.csv', index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(scale(df), index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loading vectors\n",
    "pca_loadings = pd.DataFrame(PCA().fit(X).components_.T, index=df.columns, columns=['V1', 'V2', 'V3', 'V4'])\n",
    "pca_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the PCA model and transform X to get the principal components\n",
    "pca = PCA()\n",
    "df_plot = pd.DataFrame(pca.fit_transform(X), columns=['PC1', 'PC2', 'PC3', 'PC4'], index=X.index)\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax1 = plt.subplots(figsize=(9,7))\n",
    "\n",
    "ax1.set_xlim(-3.5,3.5)\n",
    "ax1.set_ylim(-3.5,3.5)\n",
    "\n",
    "# Plot Principal Components 1 and 2\n",
    "for i in df_plot.index:\n",
    "    ax1.annotate(i, (df_plot.PC1.loc[i], -df_plot.PC2.loc[i]), ha='center')\n",
    "\n",
    "# Plot reference lines\n",
    "ax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "ax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "\n",
    "ax1.set_xlabel('First Principal Component')\n",
    "ax1.set_ylabel('Second Principal Component')\n",
    "    \n",
    "# Plot Principal Component loading vectors, using a second y-axis.\n",
    "ax2 = ax1.twinx().twiny() \n",
    "\n",
    "ax2.set_ylim(-1,1)\n",
    "ax2.set_xlim(-1,1)\n",
    "ax2.tick_params(axis='y', colors='red')\n",
    "ax2.set_xlabel('Principal Component loading vectors', color='red')\n",
    "\n",
    "# Plot labels for vectors. Variable 'a' is a small offset parameter to separate arrow tip and text.\n",
    "a = 1.07  \n",
    "for i in pca_loadings[['V1', 'V2']].index:\n",
    "    ax2.annotate(i, (pca_loadings.V1.loc[i]*a, -pca_loadings.V2.loc[i]*a), color='red')\n",
    "\n",
    "# Plot vectors\n",
    "ax2.arrow(0,0,pca_loadings.V1[0], -pca_loadings.V2[0])\n",
    "ax2.arrow(0,0,pca_loadings.V1[1], -pca_loadings.V2[1])\n",
    "ax2.arrow(0,0,pca_loadings.V1[2], -pca_loadings.V2[2])\n",
    "ax2.arrow(0,0,pca_loadings.V1[3], -pca_loadings.V2[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of the four principal components\n",
    "np.sqrt(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.plot([1,2,3,4], pca.explained_variance_ratio_, '-o', label='Individual component')\n",
    "plt.plot([1,2,3,4], np.cumsum(pca.explained_variance_ratio_), '-s', label='Cumulative')\n",
    "\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks([1,2,3,4])\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(2)\n",
    "X = np.random.standard_normal((50,2))\n",
    "X[:25,0] = X[:25,0]+3\n",
    "X[:25,1] = X[:25,1]-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km1 = KMeans(n_clusters=2, n_init=20)\n",
    "km1.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km1.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See plot for K=2 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "km2 = KMeans(n_clusters=3, n_init=20)\n",
    "km2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(km2.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of distances of samples to their closest cluster center.\n",
    "km2.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "ax1.scatter(X[:,0], X[:,1], s=40, c=km1.labels_, cmap=plt.cm.prism) \n",
    "ax1.set_title('K-Means Clustering Results with K=2')\n",
    "ax1.scatter(km1.cluster_centers_[:,0], km1.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2)\n",
    "\n",
    "ax2.scatter(X[:,0], X[:,1], s=40, c=km2.labels_, cmap=plt.cm.prism) \n",
    "ax2.set_title('K-Means Clustering Results with K=3')\n",
    "ax2.scatter(km2.cluster_centers_[:,0], km2.cluster_centers_[:,1], marker='+', s=100, c='k', linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,18))\n",
    "\n",
    "for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)], ['c1','c2','c3'],\n",
    "                                [ax1,ax2,ax3]):\n",
    "    cluster = hierarchy.dendrogram(linkage, ax=ax, color_threshold=0)\n",
    "\n",
    "ax1.set_title('Complete Linkage')\n",
    "ax2.set_title('Average Linkage')\n",
    "ax3.set_title('Single Linkage');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C: NCI60 Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI60 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two elements of this ISLR dataset are presented as csv files.\n",
    "# There is one file for the features and another file for the classes/types.\n",
    "df2 = pd.read_csv('../data/NCI60_X.csv').drop('Unnamed: 0', axis=1)\n",
    "df2.columns = np.arange(df2.columns.size)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(scale(df2))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/NCI60_y.csv', usecols=[1], skiprows=1, names=['type'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the PCA model and transform X to get the principal components\n",
    "pca2 = PCA()\n",
    "df2_plot = pd.DataFrame(pca2.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "color_idx = pd.factorize(y.type)[0]\n",
    "cmap = plt.cm.hsv\n",
    "\n",
    "# Left plot\n",
    "ax1.scatter(df2_plot.iloc[:,0], -df2_plot.iloc[:,1], c=color_idx, cmap=cmap, alpha=0.5, s=50)\n",
    "ax1.set_ylabel('Principal Component 2')\n",
    "\n",
    "# Right plot\n",
    "ax2.scatter(df2_plot.iloc[:,0], df2_plot.iloc[:,2], c=color_idx, cmap=cmap, alpha=0.5, s=50)\n",
    "ax2.set_ylabel('Principal Component 3')\n",
    "\n",
    "# Custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels).\n",
    "handles = []\n",
    "labels = pd.factorize(y.type.unique())\n",
    "norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0)\n",
    "\n",
    "for i, v in zip(labels[0], labels[1]):\n",
    "    handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5))\n",
    "\n",
    "ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# xlabel for both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel('Principal Component 1')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([df2_plot.iloc[:,:5].std(axis=0, ddof=0).values,\n",
    "              pca2.explained_variance_ratio_[:5],\n",
    "              np.cumsum(pca2.explained_variance_ratio_[:5])],\n",
    "             index=['Standard Deviation', 'Proportion of Variance', 'Cumulative Proportion'],\n",
    "             columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_plot.iloc[:,:10].var(axis=0, ddof=0).plot(kind='bar', rot=0)\n",
    "plt.ylabel('Variances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# Left plot\n",
    "ax1.plot(pca2.explained_variance_ratio_, '-o')\n",
    "ax1.set_ylabel('Proportion of Variance Explained')\n",
    "ax1.set_ylim(ymin=-0.01)\n",
    "\n",
    "# Right plot\n",
    "ax2.plot(np.cumsum(pca2.explained_variance_ratio_), '-ro')\n",
    "ax2.set_ylabel('Cumulative Proportion of Variance Explained')\n",
    "ax2.set_ylim(ymax=1.05)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel('Principal Component')\n",
    "    ax.set_xlim(-1,65)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI60 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= pd.DataFrame(scale(df2), index=y.type, columns=df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(20,20))\n",
    "\n",
    "for linkage, cluster, ax in zip([hierarchy.complete(X), hierarchy.average(X), hierarchy.single(X)],\n",
    "                                ['c1','c2','c3'],\n",
    "                                [ax1,ax2,ax3]):\n",
    "    cluster = hierarchy.dendrogram(linkage, labels=X.index, orientation='right', color_threshold=0, leaf_font_size=10, ax=ax)\n",
    "\n",
    "ax1.set_title('Complete Linkage')\n",
    "ax2.set_title('Average Linkage')\n",
    "ax3.set_title('Single Linkage');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "cut4 = hierarchy.dendrogram(hierarchy.complete(X),\n",
    "                            labels=X.index, orientation='right', color_threshold=140, leaf_font_size=10)\n",
    "plt.vlines(140,0,plt.gca().yaxis.get_data_interval()[1], colors='r', linestyles='dashed');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "km4 = KMeans(n_clusters=4, n_init=50)\n",
    "km4.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km4.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations per KMeans cluster\n",
    "pd.Series(km4.labels_).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations per Hierarchical cluster\n",
    "cut4b = hierarchy.dendrogram(hierarchy.complete(X), truncate_mode='lastp', p=4, show_leaf_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchy based on Principal Components 1 to 5\n",
    "plt.figure(figsize=(10,20))\n",
    "pca_cluster = hierarchy.dendrogram(hierarchy.complete(df2_plot.iloc[:,:5]), labels=y.type.values, orientation='right', color_threshold=100, leaf_font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut4c = hierarchy.dendrogram(hierarchy.complete(df2_plot), truncate_mode='lastp', p=4,\n",
    "                             show_leaf_counts=True)\n",
    "# See also color coding in plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Exploration: The Curse of Dimensionality\n",
    "\n",
    "The Curse of Dimensionality is a way of saying that in high-dimensional space, things behave in unexpected ways. In particular, models based largely on finding the closest distances behave poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random variation in extra dimensions can hide significant differences between clusters. This can become a serious problem with a large number of dimensions, as no point is really close to any other point and distances alone because less meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot histograms on top of each other cleanly\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as scs\n",
    "\n",
    "\n",
    "def multihist(x, y, bins=None, binsize=None, xmin=None, xmax=None, ymax=None,\n",
    "              normed=1, kde=False, alpha=0.3, figsize=(12, 8), title=None):\n",
    "    '''\n",
    "    Plot a set of overlapping histograms with identical bins\n",
    "    INPUT:\n",
    "    x:      numpy array; data points to plot\n",
    "    y:      numpy array of the same length;\n",
    "            will plot histogram for each unique value of y\n",
    "    bins:   int; number of bins in histogram\n",
    "    binsize:float: size of bins (overrides bins)\n",
    "    xmin:   lower limit (or None to set to min of data)\n",
    "    xmax:   upper limit (or None to set to max of data)\n",
    "    ymax:   upper limit of y\n",
    "    normed: normalize w each histograph; pass to matplotlib\n",
    "    kde:    add kde plot\n",
    "    alpha:  float; opacity; pass to matplotlib\n",
    "    figsize:tuple; width and height of figure; pass to matplotlib\n",
    "    title:  str; title of plot\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    if not title is None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # handle specifying xmin/xmax by clipping values to that range\n",
    "    if (xmin is None) and (xmax is None):\n",
    "        xc = x\n",
    "    else:\n",
    "        xc = np.clip(x, a_min=xmin, a_max=xmax)\n",
    "    \n",
    "    xbinmin, xbinmax = xc.min(), xc.max()\n",
    "    if xmin is not None:\n",
    "        xbinmin = min(xbinmin, xmin)\n",
    "    if xmax is not None:\n",
    "        xbinmax = max(xbinmax, xmax)\n",
    "\n",
    "    if binsize == None:\n",
    "        if bins == None:\n",
    "            bins = 20\n",
    "        binsize = (xc.max() - xc.min())/bins\n",
    "        binarray = np.linspace(xbinmin, xbinmax, bins + 1)\n",
    "    else:\n",
    "        binarray = np.arange(xbinmin, xbinmax, binsize)\n",
    "\n",
    "    if kde:\n",
    "        xvals = np.linspace(xc.min(), xc.max(), 100)\n",
    "        kde_scale = 1\n",
    "\n",
    "    # We need to get the default color cycle to get the same color\n",
    "    # for the hist and kde line.\n",
    "    props = plt.rcParams['axes.prop_cycle']\n",
    "\n",
    "    for yval, prop in zip(np.unique(y), props):\n",
    "        color = prop['color']\n",
    "        h = ax.hist(list(xc[y == yval]), alpha=alpha, bins=binarray,\n",
    "                     normed=normed, label=str(yval), color=color)\n",
    "        if kde:\n",
    "            kde_func = scs.gaussian_kde(xc[y == yval])\n",
    "            if not normed:\n",
    "                kde_scale = np.sum(y == yval) * binsize\n",
    "            ax.plot(xvals, kde_scale * kde_func(xvals), color=color)\n",
    "    ax.set_xlim(xmin=xmin, xmax=xmax)\n",
    "    ax.set_ylim(ymax=ymax)\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two clusters plotted in increasing numbers of dimensions. How much do they overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 500\n",
    "sep = 5.\n",
    "\n",
    "x = np.zeros((2, 2*count))\n",
    "x[0,:] = np.concatenate((stats.norm(-sep/2., 1).rvs(count),\n",
    "                        (stats.norm(sep/2., 1).rvs(count))))\n",
    "x[1, :] = stats.norm(0, 1).rvs(2*count)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[0, :count], x[1, :count], c='r', alpha=0.3)\n",
    "ax.scatter(x[0, count:], x[1, count:], c='b', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20, 30, 50]:\n",
    "    x = np.zeros((dims, 2*count))\n",
    "    x[0,:] = np.concatenate((stats.norm(-sep/2., 1).rvs(count),\n",
    "                            (stats.norm(sep/2., 1).rvs(count))))\n",
    "    for d in range(1, dims):\n",
    "        x[d, :] = stats.norm(0, 1).rvs(2*count)\n",
    "\n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of dimensions get large, the *every* point is far from its neighbors, even those in the same clusters. The differences between that and neighboring clusters is washed out.\n",
    "\n",
    "That said, this assumes additional dimensions add nothing. What if each additional dimension adds an equal amount of signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20]:\n",
    "    \n",
    "    x = np.zeros((dims, 2*count))\n",
    "    for d in range(0, dims):\n",
    "        x[d,:] = np.concatenate((stats.norm(-sep/2., 1).rvs(count),\n",
    "                                (stats.norm(sep/2., 1).rvs(count))))\n",
    "\n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here all the extra dimensions help, but that's not very realistic either. What if we assume that all these that each new dimension has an exponentially decreasing amount of signal.\n",
    "\n",
    "Here additional dimension helps, but only up to a point. Later dimensions that are almost all noise make things worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dims in [1, 2, 3, 5, 10, 20, 30, 50]:\n",
    "    \n",
    "    x = np.zeros((dims, 2*count))\n",
    "    for d in range(0, dims):\n",
    "        x[d,:] = np.concatenate((stats.norm(-sep/2.*(.8**d), 1).rvs(count),\n",
    "                                (stats.norm(sep/2.*(.8**d), 1).rvs(count))))\n",
    "    \n",
    "    clusterlabels = [\"cluster 1\"] * count + [\"cluster 2\"] * count \n",
    "    \n",
    "    y = []\n",
    "    dist = []\n",
    "    for i in range(2*count):\n",
    "        for j in range(i+1, 2*count):\n",
    "            d = np.sqrt(sum((x[:,i] - x[:, j])**2))\n",
    "            dist.append(d)\n",
    "            y.append(\"same\" if i // count == j // count else \"other\")\n",
    "    multihist(np.array(dist), np.array(y), binsize=0.25, figsize=(12,3),\n",
    "                   xmin=0, xmax=20, title=\"{0} dimensions\".format(dims))\n",
    "    #plt.axes(frameon=False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In high dimensions, points become extremely spare -- nothing is very near anything else, to where every point is a similar distance to every other. While you can achieve the same density in with more points, it takes a *lot* more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
