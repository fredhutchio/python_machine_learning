{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portions of today's lesson were adapted from materials developed by Jack Bennetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fredhutch.io -- Intermediate Python: Machine Learning\n",
    "Fred Hutchinson Cancer Research Center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 part 1 -- Logistic Regression in Classification\n",
    "Last week, we looked closely at using linear regression to predict a target variable. In part 1 of this week, we'll examine why linear regression isn't such a good choice for classification and investigate logistic regression as a better alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of part 1 of today's class, you should be able to:\n",
    "\n",
    " * Place logistic regression in the taxonomy of ML algorithms\n",
    " * Explain the key differences and similarities between logistic and linear regression.\n",
    " * Fit and interpret a logistic regression model in scikit-learn\n",
    " * Interpret the coefficients of logistic regression, using odds ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughout we've scattered pairs of cells like the 2 immediately below. Use them to note your thoughts, answers to questions, and the code you're experimenting with.\n",
    "(remember that you can change the type of a cell by going into command mode (cell highlighted in blue) and pressing `m` for markdown and `y` for code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression as a model\n",
    "\n",
    "Logistic regression is a **supervised-learning** **parametric** **classification** model.\n",
    "\n",
    "A supervised-learning model is one in which we predict a target (or label or $y$) based on the values of the features (or $x$s). The alternative (an unsupervised model) tries to discover something about the data without predicting a specific value.\n",
    "\n",
    "A parametric model is fit with a fixed number of parameters. Non-parametric models also have parameters but can by arbitrarily complex based on the data.\n",
    "\n",
    "Supervised-learning models are (usually) either regressors or **classifiers**. With regressors the target is a number; given values for features it will predict a numeric value.\n",
    "\n",
    "For a classifier (like logistic regression), the target is categorical. Mostly we'll discuss binary classifiers but consider multinomial classification as well. Our model will (or at least, should) predict the probability that the target would be in each possible class, rather than simply the \"best\" class. Some classifiers only predict a class rather than probabilities; these are called **hard classifiers**. In general, **soft classifiers** are more useful as they can be converted to hard classifiers if needed.\n",
    "\n",
    "Logistic regression is generally the first thing you try when building a classifier and is actually used in some production environments.\n",
    "\n",
    "Advantages:\n",
    "\n",
    " * Fast (for training and prediction)\n",
    " * Simple (few hyperparameters)\n",
    " * Interpretable\n",
    " * Provides good probabilities\n",
    " \n",
    "Disadvantages\n",
    "\n",
    " * Requires feature engineering to capture non-linear relationships\n",
    " * Doesn't work for p > n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not linear regression?\n",
    "\n",
    "Let's suppose we have some (fake) data, but that $y$ (the target) is always either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100\n",
    "\n",
    "X = stats.uniform(-9, 18).rvs(npts).reshape(npts,1)\n",
    "y = stats.bernoulli(scipy.special.expit(X[:,0])).rvs(npts)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, y, 'o', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focused previously on linear regression. Let's use that to predict $y$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "ax.plot(X, model.predict(X))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: what's wrong with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to linear regression\n",
    "\n",
    "Recall that with linear regression we created a model like\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{y} & = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... \\beta_p X_p \\\\\n",
    "  & = \\mathbf{X} \\beta\n",
    "\\end{align}$$\n",
    "\n",
    "We tried to find the \"best\" line such that all the values of $\\hat{y}$ were as close to the corresponding values of $y$ as possible. We defined this using a **loss function** that measured how bad of a job we did, and found the parameters for the line that would minimize that.\n",
    "\n",
    "For a loss function we choose the sum of squares of the residuals:\n",
    "\n",
    "$$\\text{Loss function} = \\sum_i{(\\hat{y}_i - y_i) ^ 2}$$\n",
    "\n",
    "Discussion: what loss function should we use for a classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "There are a couple different options for loss functions. Sometimes we'll talk about a **score** rather than a loss; these only differ in the sign. For a loss function, a smaller value is a better fit; for a score, a larger value is a better fit.\n",
    "\n",
    "For classifiers we almost always talk about the log loss but I'll mention the Brier score for comparison.\n",
    "\n",
    "Assume we just have two classes, so $y_i \\in \\{0, 1\\}$. Let $p_i$ be the probability the model predicts that the that $y_i$ is 1, i.e., $p_i = \\mathbb{E}[\\hat y_i]$.\n",
    "\n",
    "\n",
    "The **log loss** is then defined as\n",
    "\n",
    "$$- \\sum_i y_i \\ln{p_i} + (1-y_i) \\ln{(1-p_i)}$$\n",
    "\n",
    "**Question:** what's the log loss on a single point if the model predicts there's a 50% chance it's in one class and a 50% in the other?\n",
    "\n",
    "The **brier loss** is given by\n",
    "\n",
    "$$- \\sum_i (y_i - p_i) ^ 2 $$\n",
    "\n",
    "**Question:** what's the brier loss on a single point if the model predicts there's a 50% chance it's in one class and a 50% in the other?\n",
    "\n",
    "Note that it's much more common to talk about **brier score** than brier loss; they are just the negatives of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "xpts = np.linspace(0.01, .99, 100)\n",
    "axes[0].plot(xpts, -np.log(xpts), color='b', label=\"log loss if actual y=1\")\n",
    "axes[0].plot(xpts, -np.log(1-xpts), color='r', label=\"log loss if actual y=0\")\n",
    "axes[0].set_xlabel('predicted probability of y=1')\n",
    "axes[0].set_ylabel('log loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(xpts, (1-xpts)**2, color='b', label=\"brier loss if actual y=1\")\n",
    "axes[1].plot(xpts, (xpts)**2, color='r', label=\"brier loss if actual y=0\")\n",
    "axes[1].set_xlabel('predicted probability of y=1')\n",
    "axes[1].set_ylabel('brier loss')\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a scoring rule, we need a function to fit. When doing linear regression we chose a straight line. Here we want a function that ranges from 0 to 1.\n",
    "\n",
    "Here are a few choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "xpts = np.linspace(-6, 6, 100)\n",
    "ax.plot(xpts, 1/(1+np.exp(-xpts)), label='logistic', lw=2)\n",
    "ax.plot(xpts, stats.norm(0,1).cdf((2*np.pi)**.5/4 * xpts), label='error function/cdf of gaussian (rescaled)')\n",
    "ax.plot(xpts, np.arctan(np.pi/4*xpts)/np.pi + 0.5, label='arctan (rescaled)')\n",
    "ax.legend()\n",
    "ax.set_title(\"Assorted monotonic functions going from 0 to 1\")\n",
    "ax.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could choose any of these, but for various reasons we choose the logistic function, which is given by\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "If we substitute the $x$ with some parameters times the features, plus an intercept, and we find the values for those parameters that minimize log loss, we have logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Statistical Aside - Probability vs Likelihood\n",
    "\n",
    "Probability: Given a process for generating data, we can ask about the **probability** of getting a specific outcome.\n",
    "\n",
    "example: Given a fair six-sided die (d6), what is the probability of rolling a 5 or 6 on the first try?\n",
    "\n",
    "Likelihood: Given a specific outcome, we can ask about the **likelihood** that different processes generated the data.\n",
    "\n",
    "example: Given a sequence of coin flips, is it more likely those flips were generated using a fair coin or a 60:40 head-weighted coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE formulation\n",
    "\n",
    "\n",
    "**Question:** what is MLE? Maximum Likelihood Estimator...\n",
    "\n",
    "For logistic regression we'll change a couple things. First, we aren't going to use a line.\n",
    "\n",
    "$$\\begin{align}\n",
    "Y & = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... \\beta_p X_p + \\epsilon \\\\\n",
    "  & = \\mathbf{X} \\beta + \\epsilon\n",
    "\\end{align}$$\n",
    "\n",
    "where again\n",
    "\n",
    "$$\\epsilon \\sim N(0, \\sigma)$$\n",
    "\n",
    "\n",
    "Alternatively, we could say\n",
    "$$Y \\sim N(\\mathbf{X} \\beta, \\sigma)$$\n",
    "\n",
    "From there, we want to find the values for $\\beta$ that are the most likely to produce the data. The MLE estimate gives us ordinary least squares.\n",
    "\n",
    "Discussion: how can we use MLE to the build a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a model\n",
    "\n",
    "To do any sort of prediction we need to create a model. Since we have two possible values of $y$ we assume the data is the result of a Bernoulli distribution, where $p$ is a function of $\\mathbf{X}$.\n",
    "\n",
    "$$Y \\sim Bernoulli(f(\\mathbf{X}))$$\n",
    "\n",
    "We want our model to include some parameters so, as with linear regression, we'll have the function depend on the product of those parameters and $\\mathbf{X}$.\n",
    "\n",
    "$$Y \\sim Bernoulli(f(\\mathbf{X} \\beta))$$\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "$$Y \\sim Bernoulli(f(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... \\beta_p X_p)$$\n",
    "\n",
    "For $f$ we want a function with values from 0 to 1, what's called a sigmoid function. There are a number of different choices, but (for reasons we can get into later) the usual choice is the logistic function.\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Question: now that we have a model, how should we find the best fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we solve with maximum-likelihood estimation. Note that there isn't a close-form solution here; the computer has to solve numerically using some form of gradient descent (we can also talk about this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (with fake data)\n",
    "\n",
    "We'll generate fake data that matches the distribution exactly and try to recover the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100\n",
    "beta0 = 0.0\n",
    "beta1 = 0.5\n",
    "\n",
    "X = stats.uniform(-9, 18).rvs(npts).reshape(npts,1)\n",
    "# the expit function is the same as the logistic function\n",
    "y = stats.bernoulli(logistic(beta0 + beta1*X[:,0])).rvs(npts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We graph the data; the vertical line shows the actual decision boundary where the probability is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter = stats.uniform(-0.03,0.06).rvs(npts)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], y + jitter, s=10, alpha=0.5)\n",
    "ax.axvline(-beta0/beta1, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set C large to suppress regularization\n",
    "model = LogisticRegression(C=1000)\n",
    "model.fit(X, y)\n",
    "beta0hat = model.intercept_[0]\n",
    "beta1hat = model.coef_[0][0]\n",
    "print(\"beta0 =    {0:8.3f} beta1 =    {1:8.3f}\".format(beta0, beta1))\n",
    "print(\"beta0hat = {0:8.3f} beta1hat = {1:8.3f}\".format(beta0hat, beta1hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the curve we fitted on the data, along with the actual curve used to create the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts = np.linspace(-9, 9, 100)\n",
    "yhatpts = logistic(beta0hat + beta1hat * xpts)\n",
    "ypts = logistic(beta0 + beta1 * xpts)\n",
    "\n",
    "jitter = stats.uniform(-0.03,0.06).rvs(npts)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "ax.scatter(X[:,0], y + jitter, s=10, alpha=0.5, color='blue', label=\"generated data\")\n",
    "ax.plot(xpts, ypts, 'k', label='actual curve')\n",
    "ax.plot(xpts, yhatpts, 'r', label='fitted curve')\n",
    "ax.axhline(0.5, color='black')\n",
    "#ax.axvline(-beta0/beta1, color='black', label='actual boundary')\n",
    "ax.set_title(\"Fitted logistic curve\")\n",
    "ax.legend(loc='center right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log odds ratio\n",
    "\n",
    "In gambling people often talk about **odds**. If the local sportsball is favored to win 3-to-1 (or 3:1), that means that the ratio of the probability of winning is three times as great as the probability of losing, so they have a 75% chance of winning. Odd of 1:1 are even; they are as likely to win as the are to lose.\n",
    "\n",
    "(In gambling people always give the larger number first, so a 25% chance of winning is said to be 3:1 against. We won't do that, but will instead say the odds are 1:3.)\n",
    "\n",
    "The **odds ratio** is the ratio of the of the probability of the positive to the negative case, i.e., \n",
    "\n",
    "$$OR = \\frac{P(y=1)}{1-P(y=1)}$$\n",
    "\n",
    "Question: what's the odds ratio if $P(y=1) = 0.5$?\n",
    "\n",
    "Question: what if the probability of the positive class is 80%?\n",
    "\n",
    "Question: what if it's 20%?\n",
    "\n",
    "Question: what if it's 10%?\n",
    "\n",
    "The **log odds ratio** is just the log (base e) of that.\n",
    "\n",
    "Question: what is the log odds ratio if $P(y=1) = 0.5$\n",
    "\n",
    "Question: what if it's 80%?\n",
    "\n",
    "Question: what if it's 20%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that have to do with logistic regression? The logistic function takes the log odds of something and returns the probability. Let's explore this a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts = np.linspace(-5, 5)\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "\n",
    "ax.plot(xpts, logistic(xpts))\n",
    "\n",
    "xticks = np.arange(-5, 6, 1)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(logistic(xticks))\n",
    "\n",
    "ax.grid(color='g', linestyle='-', linewidth=1, alpha=0.5)\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-0, 1)\n",
    "ax.set_title(\"Logistic function\")\n",
    "ax.set_xlabel(\"log odds ratio\")\n",
    "ax.set_ylabel(\"probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 11)\n",
    "df = pd.DataFrame(['{:.3f}%'.format(a*100) for a in logistic(x)], index=x, columns=['probability'])\n",
    "df.index.name = 'log odds ratio'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The math\n",
    "\n",
    "If you're doing logistic regression, then the probability of something being in the positive class is\n",
    "$$P(y=1) = \\frac{1}{1+e^{-\\mathbf{X}\\beta}}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\begin{align}\n",
    "OR &= \\frac{P(y=1)}{1-P(y=1)} \\\\\n",
    "   &= \\frac{\\frac{1}{1+e^{-\\mathbf{X}\\beta}}}{1-\\frac{1}{1+e^{-\\mathbf{X}\\beta}}} \\\\\n",
    "   &= \\frac{\\frac{1}{1+e^{-\\mathbf{X}\\beta}}}{\\frac{1+e^{-\\mathbf{X}\\beta}}{1+e^{-\\mathbf{X}\\beta}}-\\frac{1}{1+e^{-\\mathbf{X}\\beta}}} \\\\\n",
    "   &= \\frac{1}{e^{-\\mathbf{X}\\beta}} \\\\\n",
    "   &= e^{\\mathbf{X}\\beta}\n",
    "\\end{align}\n",
    "$$\n",
    "so\n",
    "$$log(OR) = \\mathbf{X}\\beta$$\n",
    "\n",
    "So if you increase $X_i$ by 1, you increase $log(OR)$ by $\\beta_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary\n",
    "\n",
    "There are a number of different classification models that are worth talking about. A good model will do more than just predict to which class something belongs; it will predict the probability that it is in that class.\n",
    "\n",
    "At some point, though, we need to make a decision. To do that we choose a **threshold** at which we will place something in one class or the other. The **decision boundary** is the surface in feature space at which the probability is equal to the threshold.\n",
    "\n",
    "Suppose we have two features, $X_1$ and $X_2$, and we set the threshold equal to 0.5. That corresponds to a log odds ratio of 0 so the decision boundary is at\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0$$\n",
    "\n",
    "so\n",
    "\n",
    "$$ X_2 = \\frac{-\\beta_0}{\\beta_2} + \\frac{-\\beta_1}{\\beta_2}X_1 $$\n",
    "\n",
    "More broadly, for threshold $\\theta$, the decision boundary is the region for which\n",
    "\n",
    "$$\\theta = \\frac{1}{1+e^{-\\mathbf{X} \\beta}}$$\n",
    "\n",
    "$$\\theta + \\theta e^{-\\mathbf{X} \\beta} = 1$$\n",
    "\n",
    "$$e^{-\\mathbf{X} \\beta} = \\frac{1 - \\theta}{\\theta}$$\n",
    "\n",
    "$$\\mathbf{X} \\beta = \\ln{(\\frac{\\theta}{1 - \\theta})}$$\n",
    "\n",
    "Which again is the log odds ratio, so\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = \\ln{(\\frac{\\theta}{1 - \\theta})}$$\n",
    "\n",
    "so\n",
    "\n",
    "$$X_2 = \\frac{\\ln{(\\frac{\\theta}{1 - \\theta})} - \\beta_0}{\\beta_2} +  \\frac{-\\beta_1}{\\beta_2}X_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classes(nptses, mus, sds):\n",
    "    \"\"\"\n",
    "    Generate normally distributed points in multiple classes\n",
    "    Parameters\n",
    "    ----------\n",
    "    nptses : array_like (1-d)\n",
    "        sequence of numbers, the count of points in each class\n",
    "    mus : array_like (2-d)\n",
    "        sequence of vectors, the means of the multivariate distributions for each class\n",
    "    mus : array_like (3-d)\n",
    "        sequence of 2-d symmetric tensors, the standard deviation of the multivariate\n",
    "        distributions for each class\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : array (2-d)\n",
    "        features of generated points\n",
    "    y : array (2-d)\n",
    "        integer labels of points, starting at 0\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.zeros((0,2))\n",
    "    y = np.zeros((0,))\n",
    "    \n",
    "    for i, npts, mu, sd in zip(itertools.count(), nptses, mus, sds):\n",
    "        X = np.concatenate([X, stats.multivariate_normal(mu, sd).rvs(npts)])\n",
    "        y = np.concatenate([y, np.ones(npts)*i])\n",
    "    return X, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_classes((1000,1000),\n",
    "                       ((2.0, 2.5),\n",
    "                        (6.0, 0.0)),\n",
    "                       (((2.0, 0),\n",
    "                         (0, 3.0)),\n",
    "                        ((2.5, 0),\n",
    "                         (0, 3.0)))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def decision_boundary_x2(x, b0, b1, b2, threshold):\n",
    "    return (np.log((1 - threshold)/threshold) - b0 - x*b1 )/b2\n",
    "\n",
    "def plot_decision_boundary(X, y, model, ax=None):\n",
    "    '''plot 2-d array of points, with decision boundaries'''\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.scatter(X[:,0], X[:,1], color=np.where(y, 'g', 'b'), alpha=0.5, s=3)\n",
    "    ax.plot(*X[y==0].mean(axis=0).reshape(-1,1), color='b', marker='+', mew=4, ms=20)\n",
    "    ax.plot(*X[y==1].mean(axis=0).reshape(-1,1), color='g', marker='+', mew=4, ms=20)\n",
    "    ax.axis('equal')\n",
    "\n",
    "    xmin, xmax = X[:,0].min(), X[:,0].max()\n",
    "    xrng = np.array([2*xmin-xmax, 2*xmax-xmin])\n",
    "\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = ax.get_xlim()\n",
    "\n",
    "    beta0 = model.intercept_\n",
    "    beta1, beta2 = model.coef_[0]\n",
    "    print (beta0, beta1, beta2)\n",
    "    \n",
    "    for threshold, ls in zip(1/(1+np.exp(-np.arange(-3,4))), [':', '-.', '--', '-', '--', '-.', ':']):\n",
    "        ax.plot(xrng,\n",
    "                decision_boundary_x2(xrng, beta0, beta1, beta2, threshold),\n",
    "                color='r',\n",
    "                ls=ls,\n",
    "                label=\"{:.2f}%\".format(threshold*100))\n",
    "    ax.legend(title='threshold')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_classes((1000,1000),\n",
    "                       ((2.0, 2.5),\n",
    "                        (6.0, 0.0)),\n",
    "                       (((2.0, 0),\n",
    "                         (0, 3.0)),\n",
    "                        ((2.5, 0),\n",
    "                         (0, 3.0)))\n",
    "                       )\n",
    "model = LogisticRegression(C=1000, intercept_scaling=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) gives the count of instances based on the actual and predicted values of the target. For a binary classifier it looks like\n",
    "\n",
    "|      &nbsp;            |Predicted positive|Predicted negative |\n",
    "|--------------------|------------------|---------------|\n",
    "| **Actual positive**| true positive    | false negative|\n",
    "| **Actual negative**| false positive   | true negative |\n",
    "\n",
    "\n",
    "*True* and *false* refer to whether you are correct.\n",
    "\n",
    "*Positive* and *negative* refer to the **predicted** result.\n",
    "\n",
    "A *type-I error* is a false positive (which I remember because that phrase is more common than false negative).\n",
    "\n",
    "Accuracy $= \\frac{TP+TN}{TP+TN+FP+FN}$\n",
    "\n",
    "Sensitivity = Recall = TPR $= \\frac{TP}{TP+FN}$\n",
    "\n",
    "FPR $= \\frac{FP}{TN+FP}$\n",
    "\n",
    "Specificity $= \\frac{TN}{TN+FP}$\n",
    "\n",
    "Precision = PPV $= \\frac{TP}{TP+FP}$\n",
    "\n",
    "NPV $= \\frac{TN}{TN+FN}$\n",
    "\n",
    "![confusion matrix](../img/Confusion_Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-score\n",
    "\n",
    "$F_\\beta$ evaluates a test assuming that recall is $\\beta$ times as important as precision; it's a weighted harmonic mean of the two.\n",
    "\n",
    "$$F_\\beta = (1+\\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}{\\beta^2\\text{precision} + \\text{recall} } $$\n",
    "\n",
    "$$F_1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall} }  = \\frac{1}{\\frac{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}}{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves\n",
    "\n",
    "Any decent classification model will provide the probabilities that a data point is in one class or another. To visualize the overall goodness of a model we use a Receiver Operator Characteristic curve, which shows the TPR (a.k.a Sensitivity) and FPR (a.k.a. 1-Specificity) for various thresholds.\n",
    "\n",
    "An alternative is the Precision-recall curve, which is more appropriate when you're more interested in the positive class.\n",
    "\n",
    "Models can be compared by the Area Under the Curve (AUC) of either graph.\n",
    "\n",
    "Note: We're using a the `predict_proba` method below in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold_values(prob, y):\n",
    "    '''\n",
    "    Build dataframe of the various confusion-matrix ratios by threshold\n",
    "    from a list of predicted probabilities and actual y values\n",
    "    '''\n",
    "    df = pd.DataFrame({'prob': prob, 'y': y})\n",
    "    df.sort_values('prob', inplace=True)\n",
    "    \n",
    "    actual_p = df.y.sum()\n",
    "    actual_n = df.shape[0] - df.y.sum()\n",
    "\n",
    "    df['tn'] = (df.y == 0).cumsum()\n",
    "    df['fn'] = df.y.cumsum()\n",
    "    df['fp'] = actual_n - df.tn\n",
    "    df['tp'] = actual_p - df.fn\n",
    "\n",
    "    df['fpr'] = df.fp/(df.fp + df.tn)\n",
    "    df['tpr'] = df.tp/(df.tp + df.fn)\n",
    "    df['precision'] = df.tp/(df.tp + df.fp)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "    \n",
    "def plot_roc(ax, df):\n",
    "    ax.plot([1]+list(df.fpr), [1]+list(df.tpr), label=\"ROC\")\n",
    "    ax.plot([0,1],[0,1], 'k', label=\"random\")\n",
    "    ax.set_xlabel('fpr')\n",
    "    ax.set_ylabel('tpr')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend()\n",
    "    \n",
    "def plot_precision_recall(ax, df):\n",
    "    ax.plot(df.tpr,df.precision, label='precision/recall')\n",
    "    #ax.plot([0,1],[0,1], 'k')\n",
    "    ax.set_xlabel('recall')\n",
    "    ax.set_ylabel('precision')\n",
    "    ax.set_title('Precision/Recall Curve')\n",
    "    ax.plot([0,1],[df.precision[0],df.precision[0]], 'k', label='random')\n",
    "    ax.set_xlim(left=0,right=1)\n",
    "    ax.set_ylim(bottom=0,top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_classes((1000,1000),\n",
    "                       ((2.0, 2.5),\n",
    "                        (6.0, 0.0)),\n",
    "                       (((2.0, 0),\n",
    "                         (0, 3.0)),\n",
    "                        ((2.5, 0),\n",
    "                         (0, 3.0)))\n",
    "                       )\n",
    "model = LogisticRegression(C=1000)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1, ax3) = plt.subplots(1,3, figsize=(18,6))\n",
    "df = calculate_threshold_values(model.predict_proba(X)[:,1], y)\n",
    "plot_roc(ax0, df)\n",
    "plot_precision_recall(ax1, df)\n",
    "plot_decision_boundary(X, y, model, ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The \"random\" black line is a model that guesses the class randomly, parameterized by the probability of guessing one class or another.\n",
    "\n",
    "There's a nifty probabilistic interpretation of the AUC; you can find a discussion of it by Matt Drury in http://madrury.github.io/jekyll/update/statistics/2017/06/21/auc-proof.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 part 2 -- Bagging & Random Forests in Regression & Classification\n",
    "Last week, we used random forests as a contrast to our linear regression model. In part 2 of this week, we'll examine how decision trees, bagging, and random forests relate to one another and how we can build and interpret random forests in the context of regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By the end of part 2 of today's class, you should be able to:\n",
    "\n",
    " * Explain the relationship and difference between bagging and a random forest.\n",
    " * Explain why bagging/random forests are more accurate than a single decision tree.\n",
    " * Explain & construct a random forest (classification or regression).\n",
    " * Get feature importances from a random forest (if there is enough time).\n",
    " * Explain how OOB error is calculated and what it estimates (if there is enough time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    " * Discuss ensemble methods\n",
    " * Review bias/variance tradeoff\n",
    " * Review decision trees\n",
    " * Discuss bagging (bootstrap aggregation)\n",
    " * Discuss random forests\n",
    " * Discuss out-of-bag error (if there is enough time)\n",
    " * Discuss feature importance (if there is enough time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redundant imports left in to allow the two halves of today's lesson to be independent.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.special import comb\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer, load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Ensemble Method?\n",
    "\n",
    "In general, an **ensemble** method combines many weak models to form a strong model. We train multiple models on the data, such that each is different. They could be trained on different subsets of the data, or trained in different ways, or even be completely different types of models.\n",
    "\n",
    "Once we've done that, we need to combine the models to form a single model.\n",
    "\n",
    "**Class discussion:** how would get a single prediction from an ensemble of **regression** models?\n",
    "\n",
    "**Class discussion:** how would get a single prediction from an ensemble of **classification** models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles: Intuition\n",
    "\n",
    "Suppose we have 5 *independent* hard binary classifiers (they only give 0 or 1). If they are each 70% accurate, what's the accuracy of an ensemble of them?\n",
    "\n",
    "**Question:** what does independent mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ensemble_accuracy(n, p):\n",
    "    '''Given n independent classifiers each of p accuracy,\n",
    "    return the ensemble accuracy'''\n",
    "    ensemble_accuracy = 0\n",
    "    for k in range((n + 1) // 2, n+1):\n",
    "        ensemble_accuracy += comb(n, k) * p**k * (1-p)**(n-k)\n",
    "    return ensemble_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_ensemble_accuracy(5, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.arange(1, 55, 2)\n",
    "vfea = np.vectorize(find_ensemble_accuracy, excluded=['p'])\n",
    "ensemble_accuracies = vfea(ns, p=0.7)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ns, ensemble_accuracies, '.')\n",
    "ax.set_ylabel(\"Ensemble accuracy\")\n",
    "ax.set_xlabel(\"Number of independent 0.7-accuracy classifiers\")\n",
    "ax.set_title(\"Accuracy of an Ensemble of Independent Classifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\binom{5}{5} 0.7^5 + \\binom{5}{4} 0.7^4 0.3 + \\binom{5}{3} 0.7^3 0.3^2 \\approx 0.83 $$\n",
    "\n",
    "With 55 such classifiers we can achieve 99.9% accuracy.\n",
    "\n",
    "\n",
    "**Question:** what's the limitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Make Them Independent?\n",
    "\n",
    "If the learners are all the same, ensembles don't help.\n",
    "\n",
    "Train each learner on different subset of data.\n",
    "\n",
    "**Question:** Why is this better than a single good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "**Question:** what is bias?\n",
    "\n",
    "**Question:** what is variance?\n",
    "\n",
    "**Question:** what is the bias of an unpruned decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Classification Trees\n",
    "\n",
    "A **classification tree** is a decision tree to predicts whether a data point is in one class or another. Each branch node is a decision, choosing left or right based on the value of a certain feature. Each leaf node gives the probability that a data point is in one class or another.\n",
    "\n",
    "Let's look at a small tennis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data\n",
    "tennis_df = pd.read_csv('../data/tennis.txt', delim_whitespace=True)\n",
    "tennis_df.rename(columns={'playtennis': 'played'}, inplace=True)\n",
    "#tennis_df['played'] = tennis_df['played'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "tennis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need graphviz to generate the chart below. Note that this chart is neat, but not critical for the lesson.\n",
    "\n",
    "On MacOS or a Linux distro, it is probably easiest to run `conda install python-graphviz` from your terminal. This may also work for Windows users.\n",
    "\n",
    "You can also `pip install graphviz`, although you might have to manually change your `PATH` to include the graphviz executables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='A simple classification tree')\n",
    "\n",
    "dot.node('O', 'outlook?', shape='diamond')\n",
    "dot.node('1', \"no\", shape='rectangle')\n",
    "dot.node('H', 'humidity?', shape='diamond')\n",
    "dot.node('O2', 'outlook?', shape='diamond')\n",
    "dot.node('W', 'wind?', shape='diamond')\n",
    "dot.node('3', 'yes', shape='rectangle')\n",
    "dot.node('T', 'temperature?', shape='diamond')\n",
    "dot.node('4', 'yes', shape='rectangle')\n",
    "dot.node('5', \"no\", shape='rectangle')\n",
    "dot.node('2', \"no\", shape='rectangle')\n",
    "dot.node('W2', 'wind?', shape='diamond')\n",
    "dot.node('6', \"no\", shape='rectangle')\n",
    "dot.node('7', 'yes', shape='rectangle')\n",
    "\n",
    "dot.edge('O', '1', 'overcast')\n",
    "dot.edge('O', 'H', 'not overcast')\n",
    "dot.edge('H', 'O2', 'high')\n",
    "dot.edge('H', 'W', 'normal')\n",
    "dot.edge('W', '3', 'False')\n",
    "dot.edge('W', 'T', 'True')\n",
    "dot.edge('T', '4', 'mild')\n",
    "dot.edge('T', '5', 'cool')\n",
    "dot.edge('O2', '2', 'sunny')\n",
    "dot.edge('O2', 'W2', 'rainy')\n",
    "dot.edge('W2', '7', 'False')\n",
    "dot.edge('W2', '6', 'True')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification tree is built by\n",
    "\n",
    "* Iteratively divide the nodes such that (entropy/gini impurity) is minimized\n",
    "* Various stopping conditions like a depth limit\n",
    "* Prune trees by merging nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Regression Trees\n",
    "\n",
    "A **regression tree** predicts a number rather than the probability that something is in one class or another. Prediction works the same as with classification trees, but the leaf nodes give a number rather than probabilities of a class.\n",
    "\n",
    "To train a regression tree, we\n",
    "\n",
    "* Iteratively divide the nodes such that *total squared error* is minimized,\n",
    "\n",
    "$$\\sum_{i \\in L} (y_i - m_L)^2 + \\sum_{i\\in R} (y_i - m_R)^2$$\n",
    "\n",
    "* Use various stopping conditions like a depth limit, minimum leaf size, and\n",
    "* Prune trees by merging nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees: Example\n",
    "\n",
    " $x_1$ |   $x_2$ |  $y$\n",
    "-------|---------|--------\n",
    " 1     |    1    |   1\n",
    " 0     |    0    |   2\n",
    " 1     |    0    |   3\n",
    " 0     |    1    |   4\n",
    "\n",
    " Prior to the split we guess the mean, 2.5, for everything, giving total squared error:\n",
    " \n",
    " $$ E = (1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2  = 5$$\n",
    " After we split on $x_1$ we guess 2 for rows 1 & 3 and 3 for rows 2 & 4:\n",
    " \n",
    " $$ E = (1-2)^2 + (3-2)^2 + (2-3)^2 + (4-3)^2 = 4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Summary\n",
    "\n",
    "What are the pros and cons?\n",
    "\n",
    "Pros\n",
    " * No feature scaling needed\n",
    " * Model nonlinear relationships\n",
    " * Can do both classification and regression\n",
    " * Robust\n",
    " * Highly interpretable\n",
    "\n",
    "Cons\n",
    " * Can be expensive to train\n",
    " * Often poor predictors because of high variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Look: Bootstrapping\n",
    "\n",
    "What is a bootstrap sample?\n",
    "\n",
    "What are bootstrap samples are good for?\n",
    "\n",
    "Let's get the median of some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stats.uniform(0,10).rvs(100)\n",
    "np.median(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the confidence interval of this estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(data, function, alpha=0.05, n_bootstraps=1000):\n",
    "    '''return a the confidence interval for a function of data using bootstrapping'''\n",
    "    medians = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        bootstrap_sample = np.random.choice(data, len(data))\n",
    "        medians.append(function(bootstrap_sample))\n",
    "    return (np.percentile(medians, 100*(alpha/2.)),\n",
    "            np.percentile(medians, 100*(1-alpha/2.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .05\n",
    "ci = bootstrap_confidence_interval(data, np.median, alpha)\n",
    "\n",
    "print(\"The {}% confidence interval is from {} to {}\".format(1-alpha, *ci))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our procedure was\n",
    "  * Take 1000 bootstrap samples.\n",
    "  * Take the median of each sample.\n",
    "  * The 95% confidence inverval for the median is between the 25th and 975th largest samples.\n",
    "  \n",
    "To confirm this worked, let's do it a bunch of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = []\n",
    "for _ in range(100):\n",
    "    data = stats.uniform(0,10).rvs(100)\n",
    "    ci = bootstrap_confidence_interval(data, np.median)\n",
    "    hits.append(ci[0] < 5.0 and ci[1] > 5.0)\n",
    "print(np.mean(hits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't work every time, but it will if we do it a lot longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapped aggregation\n",
    "\n",
    " * We are thinking about the population of all possible decision tree models on our data.\n",
    " * If I take $n$ *iid* (independent and identically distributed) samples from this distribution and average them the variance goes down by $\\sqrt n$\n",
    " * There is some correlation between models because they are all trained on bootstrap samples from the same draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Experiment\n",
    "\n",
    "You're each going to be a decision tree on some data based on a bootstrap sample, and then we'll all ensemble the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "\n",
    "# Split into test/train, using the same random state for everyone\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,\n",
    "                                                    data.target,\n",
    "                                                    random_state=462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "# each of you has a different bootstrap sample\n",
    "bootstrap_sample = np.random.choice(range(len(X_train)),\n",
    "                                    len(X_train),\n",
    "                                    replace=True)\n",
    "clf.fit(X_train[bootstrap_sample], y_train[bootstrap_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = {:.3f}\".format(np.mean(clf.predict(X_test) == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"My prediction: {}\".format(clf.predict(X_test)[0:20]))\n",
    "print(\"Actual result: {}\".format(y_test[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept, creating each model from a bootstrap sample and aggregating the results, is called **bagging**. It could really be used with any sort of model, but is generally done with decision trees.\n",
    "\n",
    "**Question:** why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping on Regression Trees\n",
    "\n",
    "Let's do another example. Consider some points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5000\n",
    "x = stats.uniform(0, 10).rvs(n_data)\n",
    "y = np.cos(x) + stats.norm(0, 0.5).rvs(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.plot(x, y, '.', ms=2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 5\n",
    "model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "model.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pts = 1000\n",
    "xpts = np.linspace(0, 10, n_pts)\n",
    "p1 = ax.plot(xpts, model.predict(xpts.reshape(-1, 1)), 'b')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what one decision tree looks like; each discontinuity is a node split. What if we create a bunch of bootstrap samples and build an ensemble of trees from them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 5000\n",
    "yptses = np.zeros((n_bootstraps, n_pts))\n",
    "for i in range(n_bootstraps):\n",
    "    bootstrap = np.random.choice(np.arange(n_data), n_data, True)\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    model.fit(x[bootstrap].reshape(-1, 1), y[bootstrap])\n",
    "    yptses[i] = model.predict(xpts.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(xpts, yptses.mean(axis=0), 'r')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Bagging decision trees are pretty cool, but the trees still tend to look pretty similar. We want a way to make the trees more different (decorrelate them) without substantially increasing the bias of each tree.\n",
    "\n",
    "Random forests do this with **subspace sampling**. When we are building a tree and considering the feature to use at each split, we only consider a few of them, randomly chosen. The number of features $m$ to consider at each split is a hyperparameter; typically $m = \\sqrt k$ is used.\n",
    "\n",
    "Again, the features to consider are chosen **at each split**, not each tree. **Everyone gets this wrong.**\n",
    "\n",
    "**Question:** are features to consider chosen for each tree, or at each split?\n",
    "\n",
    "For example, suppose we're building a model with nine features. One of them is really predictive, another is pretty good, and the others are just okay.\n",
    "\n",
    "If we build an ensemble of bagged trees, probably each will use the good feature as the first split, and probably each will use the pretty-good feature at the next split. For the other splits the trees might differ, particularly father down when only a few points are being considered, but the first branches will be pretty much the same.\n",
    "\n",
    "If we build a trees in a random forest, we'll only consider three (random) features for that first split. Only a fraction of the trees (around 30%) will consider the good feature on the first split, so they will use that. Some of the others will consider the pretty-good feature, so they will start there. The others will start at some other feature. Those trees will still consider the good and pretty-good features at some of the lower nodes, and will get to take advantage of them, but the overall structure of those trees will be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Parameters\n",
    "\n",
    "Random Forest Parameters\n",
    "\n",
    " * Total number of trees\n",
    " * Number of features to use at each split\n",
    " * Individual decision tree Parameters\n",
    "    - e.g., tree depth, pruning, split criterion\n",
    "\n",
    "In general, RF are fairly robust to the choice of parameters and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Random Forest\n",
    "\n",
    "Pros\n",
    "\n",
    " * Often give near state-of-the-art performance\n",
    " * Good out-of-the-box performance\n",
    " * No feature scaling needed\n",
    " * Model nonlinear relationships\n",
    "\n",
    "Cons\n",
    "\n",
    " * Can be expensive to train (though can be done in parallel)\n",
    " * Not interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Decision Trees\n",
    "\n",
    "Let's investigate the accuracy of a random forests compared with a single decision tree using the breast cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer data\n",
    "data = load_breast_cancer()\n",
    "#data = pd.drop(data, )\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "X = X.drop(['worst concave points', 'mean concave points', 'worst perimeter', 'worst radius', 'worst area', 'mean concavity'], axis=1)\n",
    "# Split into test/train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.33,\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, consider a decision tree, doing a grid search over hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Search                                     \n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "depth_parm = np.arange(1, 12, 1)\n",
    "num_samples_parm = np.arange(5,95,10)\n",
    "parameters = {'max_depth' : depth_parm,\n",
    "             'min_samples_leaf' : num_samples_parm}\n",
    "\n",
    "scorer = make_scorer(log_loss,\n",
    "                     greater_is_better=False,\n",
    "                     needs_proba=True)\n",
    "clf = GridSearchCV(model,\n",
    "                   parameters,\n",
    "                   cv=10,\n",
    "                   scoring=scorer)\n",
    "clf.fit(X_train,y_train)\n",
    "print(f\"log loss = {-clf.score(X_test, y_test)}\")\n",
    "print(f\"accuracy = {(clf.predict(X_test) == y_test).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and fit model                                                   \n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                           max_features='auto',\n",
    "                           random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "                                     \n",
    "# Test Prediction\n",
    "pred = rf.predict(X_test)\n",
    "print(f\"log loss = {log_loss(y_test, rf.predict_proba(X_test)[:, 1])}\")\n",
    "print(f\"accuracy = {rf.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Material\n",
    "\n",
    "## Interpreting Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "Previous Objectives\n",
    "\n",
    " * Explain & construct a random forest (classification or regression).\n",
    " * Explain the relationship and difference between random forest and bagging.\n",
    " * Explain why random forests are more accurate than a single decision tree.\n",
    "\n",
    "Additional Objectives\n",
    "\n",
    " * Get feature importances from a random forest.\n",
    " * Explain how OOB error is calculated and what is it an estimate of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "Previously\n",
    "\n",
    " * Discuss ensemble methods\n",
    " * Review bias/variance tradeoff\n",
    " * Review decision trees\n",
    " * Discuss bagging (bootstrap aggregation)\n",
    " * Discuss random forests\n",
    "\n",
    "Additional\n",
    "\n",
    " * Discuss out-of-bag error\n",
    " * Discuss feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Bagging and Random Forests\n",
    "\n",
    "What is bagging?\n",
    "\n",
    "Can bagging be used with other models?\n",
    "\n",
    "What's the difference between bagging and random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-Of-Bag Error\n",
    "\n",
    "The out-of-bag error is a way to measure the error of a bagged model (including random forests).\n",
    "\n",
    "Since the decision trees are constructed from a bootstrap sample, each tree will (probably) not see all of the data, so each datum will (probably) not be seen by many of the trees.\n",
    "\n",
    "Let's imagine a data set with 10 points, and that we're making a random forest with 20 trees. We'll need 20 trees and construct 20 bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "n_trees = 100000\n",
    "\n",
    "np.random.seed(42)\n",
    "bootstrap_samples = np.random.choice(range(n_data),\n",
    "                                     [n_data, n_trees],\n",
    "                                     replace=True)\n",
    "# sort along columns to make it a bit easier to read\n",
    "bootstrap_samples.sort(axis=0)\n",
    "bootstrap_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then construct 20 trees, each using the data in one of the columns.\n",
    "\n",
    "Notice the first tree (and several others) isn't constructed using point 0 at all. So we could test that point of an ensemble of just those trees and get a independent measure of the effectiveness of the model. Similarly, we could test other points on other trees.\n",
    "\n",
    "**Question:** which trees would we use to test point 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where((bootstrap_samples != 7).all(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, how many trees can we use to test each point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = []\n",
    "for i in range(10):\n",
    "    bsize.append(sum((bootstrap_samples != i).all(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bsize)/n_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out this number is $\\frac{1}{e}$; you can [read about the math](https://stats.stackexchange.com/questions/88980/why-on-average-does-each-bootstrap-sample-contain-roughly-two-thirds-of-observat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we use cross validation anyway because we're comparing with other models and want to measure the accuracy the same way.\n",
    "\n",
    "\n",
    "\n",
    "## Feature Importances\n",
    "\n",
    "One of the challenges of random forests is the lack of interpretability. Feature importances are a measure of which features actually effect the predictions.\n",
    "\n",
    "This can be a critical business question. For example, with churn analysis, it's generally more important to understand *why* customers are churning than to predict which customers are going to churn.\n",
    "\n",
    "How should we measure it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances: Mean Decrease Impurity\n",
    "\n",
    "How much does each feature decrease the impurity?\n",
    "\n",
    "To compute the importance of the $j^{th}$ feature:\n",
    "\n",
    " * For each tree, each split is made in order to reduce the total impurity of the tree (Gini/entropy/MSE); we can record the magnitude of the reduction.\n",
    " * Then the importance of a feature is the average decrease in impurity across trees in the forest, as a result of splits defined by that feature.  \n",
    " * This is implemented in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances: Mean Decrease Accuracy\n",
    "\n",
    "How much does randomly mixing values of a feature affect accuracy?\n",
    "\n",
    "To compute the importance of the $j^{th}$ feature:\n",
    "\n",
    " * When the $b^{th}$ tree is grown, use it to predict the OOB samples and record accuracy.\n",
    " * Scramble the values of the $j^{th}$ feature in the OOB samples and do the prediction again.  Compute the new (lower) accuracy.\n",
    " * Average the decrease in accuracy across all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances: ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "feat_scores = pd.DataFrame({'Fraction of Samples Affected' : rf.feature_importances_},\n",
    "                           index=X.columns)\n",
    "feat_scores = feat_scores.sort_values(by='Fraction of Samples Affected')\n",
    "feat_scores.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Decrease Accuracy\n",
    "\n",
    "A different approach to calculating feature importances shuffles the values of a feature, and measures the decrease in accuracy (code taken from sklearn documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "scores = defaultdict(list)\n",
    "\n",
    "\n",
    "names = data.feature_names\n",
    " \n",
    "rf = RandomForestRegressor()\n",
    "scores = defaultdict(list)\n",
    " \n",
    "# crossvalidate the scores on a number of \n",
    "# different random splits of the data\n",
    "splitter = ShuffleSplit(100, test_size=.3)\n",
    "\n",
    "for train_idx, test_idx in splitter.split(X, y):\n",
    "    X_train, X_test = X.values[train_idx], X.values[test_idx]\n",
    "    y_train, y_test = y.values[train_idx], y.values[test_idx]\n",
    "    rf.fit(X_train, y_train)\n",
    "    acc = r2_score(y_test, rf.predict(X_test))\n",
    "    for i in range(X.shape[1]):\n",
    "        X_t = X_test.copy()\n",
    "        np.random.shuffle(X_t[:, i])\n",
    "        shuff_acc = r2_score(y_test, rf.predict(X_t))\n",
    "        scores[names[i]].append((acc-shuff_acc)/acc)\n",
    "\n",
    "score_series = pd.DataFrame(scores).mean()\n",
    "scores = pd.DataFrame({'Mean Decrease Accuracy' : score_series})\n",
    "scores.sort_values(by='Mean Decrease Accuracy').plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Notes (markdown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch area (code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
